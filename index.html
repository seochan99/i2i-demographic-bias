<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <meta name="title" content="Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing">
  <meta name="description" content="We expose demographic-conditioned failures in I2I portrait editing—Soft Erasure and Stereotype Replacement—showing pervasive skin lightening and identity drift, and propose a prompt-level mitigation that reduces bias without model updates.">
  <meta name="keywords" content="demographic bias, image-to-image editing, portrait editing, fairness, soft erasure, stereotype replacement, identity preservation, responsible AI">
  <meta name="author" content="Huichan Seo, Minki Hong, Sieun Choi, Jihie Kim, Jean Oh">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:locale" content="en_US">
  <meta property="og:site_name" content="Evaluating Demographic Misrepresentation in I2I Portrait Editing">
  <meta property="og:title" content="Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing">
  <meta property="og:description" content="We expose demographic-conditioned failures in I2I portrait editing—Soft Erasure and Stereotype Replacement—and propose a prompt-level identity-preserving mitigation.">
  <meta property="og:url" content="https://seochan99.github.io/i2i-demographic-bias/">
  <meta property="og:image" content="https://seochan99.github.io/i2i-demographic-bias/static/images/figure0.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Qualitative examples of demographic-conditioned failures in I2I editing.">
  <meta property="article:published_time" content="2026-01-30T00:00:00Z">
  <meta property="article:author" content="Huichan Seo">
  <meta property="article:section" content="Responsible AI">
  <meta property="article:tag" content="Responsible AI">
  <meta property="article:tag" content="Demographic Bias">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing">
  <meta name="twitter:description" content="We expose demographic-conditioned failures in I2I portrait editing and propose a prompt-level identity-preserving mitigation.">
  <meta name="twitter:image" content="https://seochan99.github.io/i2i-demographic-bias/static/images/figure0.png">
  <meta name="twitter:image:alt" content="Qualitative examples of demographic-conditioned failures in I2I editing.">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing">
  <meta name="citation_author" content="Seo, Huichan">
  <meta name="citation_author" content="Hong, Minki">
  <meta name="citation_author" content="Choi, Sieun">
  <meta name="citation_author" content="Kim, Jihie">
  <meta name="citation_author" content="Oh, Jean">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="Preprint">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#0d9488">
  <meta name="msapplication-TileColor" content="#0d9488">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <link rel="canonical" href="https://seochan99.github.io/i2i-demographic-bias/">
  <title>Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing</title>

  <!-- Critical CSS -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">

  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer JS -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing",
    "description": "We expose demographic-conditioned failures in I2I portrait editing—Soft Erasure and Stereotype Replacement—and propose a prompt-level identity-preserving mitigation.",
    "author": [
      {"@type": "Person", "name": "Huichan Seo", "affiliation": {"@type": "Organization", "name": "Carnegie Mellon University"}},
      {"@type": "Person", "name": "Minki Hong", "affiliation": {"@type": "Organization", "name": "Dongguk University"}},
      {"@type": "Person", "name": "Sieun Choi", "affiliation": {"@type": "Organization", "name": "Dongguk University"}},
      {"@type": "Person", "name": "Jihie Kim", "affiliation": {"@type": "Organization", "name": "Dongguk University"}},
      {"@type": "Person", "name": "Jean Oh", "affiliation": {"@type": "Organization", "name": "Carnegie Mellon University"}}
    ],
    "datePublished": "2026-01-30",
    "publisher": {"@type": "Organization", "name": "arXiv"},
    "keywords": ["demographic bias", "image-to-image editing", "portrait editing", "soft erasure", "stereotype replacement", "identity preservation"],
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/"
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- Project Resources Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View key resources for this project">
      <i class="fas fa-flask"></i>
      Project Resources
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>Explore the Project</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <div class="work-item" style="opacity: 0.6; cursor: not-allowed;">
          <div class="work-info">
            <h5>arXiv Preprint</h5>
            <p>Full paper with methodology, results, and appendices.</p>
            <span class="work-venue">arXiv - Coming Soon</span>
          </div>
          <i class="fas fa-lock"></i>
        </div>
        <div class="work-item" style="opacity: 0.6; cursor: not-allowed;">
          <div class="work-info">
            <h5>Code & Evaluation Assets</h5>
            <p>Reproducible pipelines, prompts, and evaluation code.</p>
            <span class="work-venue">GitHub - Coming Soon</span>
          </div>
          <i class="fas fa-lock"></i>
        </div>
        <div class="work-item" style="opacity: 0.6; cursor: not-allowed;">
          <div class="work-info">
            <h5>Benchmark Data</h5>
            <p>5,640 edited outputs, VLM scores, and human annotations.</p>
            <span class="work-venue">Hugging Face - Coming Soon</span>
          </div>
          <i class="fas fa-lock"></i>
        </div>
      </div>
    </div>
  </div>

  <main id="main-content">

  <!-- Hero -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://huichanseo.com/" target="_blank" rel="noopener">Huichan Seo</a><sup>1*</sup>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://bk123477.github.io/" target="_blank" rel="noopener">Minki Hong</a><sup>2*</sup>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://linkedin.com/in/sieunchoi/" target="_blank" rel="noopener">Sieun Choi</a><sup>2*</sup>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/jihiekim/" target="_blank" rel="noopener">Jihie Kim</a><sup>2</sup>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~jeanoh/" target="_blank" rel="noopener">Jean Oh</a><sup>1</sup>
              </span>
            </div>
            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>Carnegie Mellon University &middot; <sup>2</sup>Dongguk University</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution</small></span>
            </div>
            <div class="publication-links">
              <span class="link-block has-tooltip has-tooltip-arrow has-tooltip-bottom" data-tooltip="Coming soon">
                <button class="external-link button is-normal is-rounded is-dark is-disabled" disabled>
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </button>
              </span>
              <span class="link-block has-tooltip has-tooltip-arrow has-tooltip-bottom" data-tooltip="Coming soon">
                <button class="external-link button is-normal is-rounded is-dark is-disabled" disabled>
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code &amp; Data</span>
                </button>
              </span>
              <span class="link-block has-tooltip has-tooltip-arrow has-tooltip-bottom" data-tooltip="Coming soon">
                <button class="external-link button is-normal is-rounded is-dark is-disabled" disabled>
                  <span class="icon" aria-hidden="true">&#x1F917;</span>
                  <span>Hugging Face</span>
                </button>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Hero -->

  <!-- Teaser: figure0.png -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <figure class="image">
          <img src="static/images/figure0.png" alt="Qualitative examples of demographic-conditioned failures in I2I editing across different prompts and source demographics." loading="lazy">
        </figure>
        <h2 class="subtitle has-text-centered mt-4">
          Identical edit instructions yield systematically different outcomes across subject demographics: skin lightening, race change, and gender inference reveal deeply embedded priors in open-weight I2I editors.
        </h2>
      </div>
    </div>
  </section>
  <!-- End Teaser -->

  <!-- Overall Framework: figure2.png -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <h2 class="title is-3 has-text-centered">Overall Framework</h2>
        <div class="columns is-centered">
          <div class="column is-10">
            <div class="box has-background-white">
              <figure class="image">
                <img src="static/images/figure2.png" alt="Overview of the study framework: source portraits, diagnostic prompts, three I2I editors, feature prompt mitigation, VLM and human evaluation." loading="lazy">
              </figure>
              <p class="is-size-7 has-text-grey mt-2 has-text-centered">
                We build a controlled benchmark from FairFace, pair source portraits with diagnostic prompts, run three I2I editing models, and assess outputs via human evaluation and a VLM ensemble. For feature prompt mitigation, we prepend identity-preserving constraints and re-run editing under identical conditions.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Overall Framework -->

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: <em>Soft Erasure</em>, where edits are silently weakened or ignored in the output image, and <em>Stereotype Replacement</em>, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Abstract -->

  <!-- Image Carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-landscape">
            <img src="static/images/figure2.png" alt="Pipeline overview: source portraits paired with edit prompts, three I2I editors, and VLM + human evaluation." loading="lazy">
            <h2 class="subtitle has-text-centered">
              End-to-end evaluation pipeline: FairFace source portraits paired with diagnostic prompts, edited by three open-weight I2I editors, and assessed via VLM ensemble and human evaluation.
            </h2>
          </div>
          <div class="item item-landscape">
            <img src="static/images/figure4.png" alt="Qualitative comparison of baseline vs. feature prompt outputs showing reduced race change." loading="lazy">
            <h2 class="subtitle has-text-centered">
              Qualitative comparison of baseline and ours. Feature prompts reduce race change for non-White subjects by preserving source identity attributes.
            </h2>
          </div>
          <div class="item item-landscape">
            <img src="static/images/figure5.png" alt="Gender-occupation stereotypes: WinoBias-based edits show models adopt stereotype-consistent gender presentations." loading="lazy">
            <h2 class="subtitle has-text-centered">
              WinoBias-derived occupation edits reveal 84-86% stereotype adherence: models override source gender with occupation-driven priors.
            </h2>
          </div>
          <div class="item item-landscape">
            <img src="static/images/appendix_per_prompt_grid.png" alt="Same prompt applied across seven racial groups showing systematic skin lightening for darker-skinned subjects." loading="lazy">
            <h2 class="subtitle has-text-centered">
              Cross-race comparison: identical prompts produce consistent skin lightening for darker-skinned subjects and stereotype-congruent feature enhancement.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Carousel -->

  <!-- Research Contributions -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <h2 class="title is-3 has-text-centered">Research Contributions</h2>
        <div class="columns is-multiline is-variable is-5">
          <div class="column is-half-tablet is-one-quarter-desktop is-flex">
            <div class="box has-text-left is-flex is-flex-direction-column is-flex-grow-1">
              <h3 class="title is-5">Failure Mode Formalization</h3>
              <p class="mt-2 is-flex-grow-1">We identify and define two demographic-conditioned failure modes in I2I editing: <em>Soft Erasure</em> (silent edit suppression) and <em>Stereotype Replacement</em> (unrequested identity change driven by social priors).</p>
            </div>
          </div>
          <div class="column is-half-tablet is-one-quarter-desktop is-flex">
            <div class="box has-text-left is-flex is-flex-direction-column is-flex-grow-1">
              <h3 class="title is-5">Controlled Benchmark</h3>
              <p class="mt-2 is-flex-grow-1">5,040 edited images from 84 factorially sampled FairFace portraits across race, gender, and age, evaluated by a dual-VLM ensemble and 30 human annotators on Prolific.</p>
            </div>
          </div>
          <div class="column is-half-tablet is-one-quarter-desktop is-flex">
            <div class="box has-text-left is-flex is-flex-direction-column is-flex-grow-1">
              <h3 class="title is-5">Prompt-level Mitigation</h3>
              <p class="mt-2 is-flex-grow-1">A feature prompt that specifies observable appearance attributes reduces identity change for non-White groups by up to 1.48 points without any model modification.</p>
            </div>
          </div>
          <div class="column is-half-tablet is-one-quarter-desktop is-flex">
            <div class="box has-text-left is-flex is-flex-direction-column is-flex-grow-1">
              <h3 class="title is-5">VLM-Human Alignment</h3>
              <p class="mt-2 is-flex-grow-1">Strong agreement between VLM scoring and human evaluation validates automated assessment as a scalable, conservative lower bound on demographic-conditioned failures.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Contributions -->

  <!-- Failure Modes: failure_example.png -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-vcentered is-variable is-6">
          <div class="column is-5">
            <h2 class="title is-3">Two Failure Modes in I2I Editing</h2>
            <p><strong>Soft Erasure</strong> occurs when the editor silently suppresses the requested edit, yielding unchanged or minimally altered results despite producing an output image.</p>
            <p class="mt-3"><strong>Stereotype Replacement</strong> occurs when edits introduce stereotype-consistent demographic attributes not specified in the prompt&mdash;skin lightening, race change, or gender inference driven by occupational priors.</p>
            <p class="mt-3 is-size-6 has-text-grey">These failures are not reliably captured by generic edit-quality metrics, motivating our multi-axis evaluation protocol.</p>
          </div>
          <div class="column is-7">
            <figure class="image">
              <img src="static/images/failure_example.png" alt="Examples of Soft Erasure (edit ignored) and Stereotype Replacement (identity changed toward majority-group features)." loading="lazy">
            </figure>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Failure Modes -->


  <!-- Racial Disparity: exp1_race_disparity.png -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-vcentered is-variable is-6">
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/exp1_race_disparity.png" alt="Racial disparities in skin lightening (72-75% for Indian/Black vs 44% for White) and race change (14% for Indian vs 1% for White)." loading="lazy">
            </figure>
          </div>
          <div class="column is-half">
            <h2 class="title is-3">Pervasive Racial Disparity</h2>
            <p><strong>62&ndash;71% of all edited outputs exhibit lighter skin tones</strong> than the source image. This effect is not uniform: Indian and Black subjects experience 72&ndash;75% skin lightening, compared to 44% for White subjects.</p>
            <p class="mt-3">Race change shows even starker disparity: Indian subjects experience 14% change vs. only 1% for White subjects. This systematic drift toward lighter skin and White-presenting features occurs across <strong>all three models and all prompt categories</strong>.</p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Racial Disparity -->

  <!-- Feature Prompt Mitigation: figure4.png -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-variable is-6 is-vcentered">
          <div class="column is-5">
            <h2 class="title is-3">Asymmetric Mitigation via Feature Prompts</h2>
            <p>Without any model modification, prepending observable appearance features to edit instructions reduces identity change across all non-White groups.</p>
            <p class="mt-3">Feature prompts reduce race change by <strong>1.48 points for Black subjects</strong> but only 0.06 points for White subjects. This asymmetry reveals a &ldquo;default to White&rdquo; prior: without constraints, edits drift toward White-presenting outputs.</p>
            <p class="mt-3 is-size-6 has-text-grey">The mitigation operates purely at the prompt level&mdash;model-agnostic, no fine-tuning required, applicable to closed-source editors.</p>
          </div>
          <div class="column is-7">
            <div class="box has-background-white" style="overflow-x: auto;">
              <figure class="image">
                <img src="static/images/figure4.png" alt="Qualitative comparison: baseline outputs show identity drift for non-White subjects; feature prompt outputs preserve identity." loading="lazy">
              </figure>
              <p class="is-size-7 has-text-grey mt-2 has-text-centered">Feature prompts reduce race change for non-White subjects by preserving source identity attributes. Edit success decreases as the model prioritizes identity preservation over edit compliance.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Mitigation -->

  <!-- Human Evaluation Platform -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <h2 class="title is-3 has-text-centered">Human Evaluation Platform</h2>
        <p class="has-text-centered mb-5">We recruited N=30 annotators via Prolific to validate VLM-based scoring. Each output was independently rated by three human raters using the same 5-point rubric, yielding 3,000 annotations.</p>
        <div class="columns is-multiline is-variable is-4">
          <div class="column is-half">
            <div class="box has-background-white">
              <figure class="image">
                <img src="static/images/web-guide.png" alt="Onboarding guide showing evaluation task structure with source image, two AI-edited outputs, and edit prompt." loading="lazy">
              </figure>
              <p class="is-size-7 has-text-grey mt-2 has-text-centered">Onboarding guide: participants are shown the evaluation task structure with source image, two edited outputs, and the edit prompt.</p>
            </div>
          </div>
          <div class="column is-half">
            <div class="box has-background-white">
              <figure class="image">
                <img src="static/images/web-task-select.png" alt="Main evaluation interface with side-by-side comparison and 5-point Likert scales." loading="lazy">
              </figure>
              <p class="is-size-7 has-text-grey mt-2 has-text-centered">Main evaluation interface: side-by-side comparison with independent 5-point Likert scales for each evaluation dimension.</p>
            </div>
          </div>
          <div class="column is-half">
            <div class="box has-background-white">
              <figure class="image">
                <img src="static/images/web-IRB.png" alt="IRB consent form confirming study purpose and participant anonymity." loading="lazy">
              </figure>
              <p class="is-size-7 has-text-grey mt-2 has-text-centered">IRB consent form: participants confirm eligibility and agree to participate in the study.</p>
            </div>
          </div>
          <div class="column is-half">
            <div class="box has-background-white">
              <figure class="image">
                <img src="static/images/web-task-dashboard-done.png" alt="Task selection dashboard showing completed and available tasks." loading="lazy">
              </figure>
              <p class="is-size-7 has-text-grey mt-2 has-text-centered">Task dashboard: participants complete one task of 100 items, with completion status tracked in real time.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Human Evaluation -->

  <!-- VLM-Human Alignment: vlm_human_validation.png -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-vcentered is-variable is-6">
          <div class="column is-half">
            <h2 class="title is-3">VLM-Human Alignment</h2>
            <p>Human scores detect significant racial differences in skin tone (Kruskal&ndash;Wallis <em>H</em> = 24.7, <em>p</em> &lt; 0.001) and a White vs. Non-White disparity (Mann&ndash;Whitney <em>U</em>, <em>p</em> = 0.020), matching VLM-identified patterns.</p>
            <p class="mt-3">VLM systematically <strong>overestimates edit success</strong> by +0.72 points on average, meaning VLM-detected soft erasure rates serve as a conservative lower bound on the true prevalence.</p>
            <p class="mt-3 is-size-6 has-text-grey">Identity drift differences between VLM and human means are small (race: 0.03&ndash;0.16; gender: 0.05&ndash;0.12; age: 0.02&ndash;0.10), suggesting VLM scoring enables reliable, scalable assessment.</p>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/vlm_human_validation.png" alt="Human evaluation: (a) mean skin tone scores by race showing significant racial disparity; (b) edit success and change reduction with feature prompts." loading="lazy">
            </figure>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End VLM-Human -->

  <!-- Gender-Occupation Stereotypes: figure5.png -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-variable is-6 is-vcentered">
          <div class="column is-7">
            <div class="box has-background-white" style="overflow-x: auto;">
              <figure class="image">
                <img src="static/images/figure5.png" alt="Gender-occupation stereotypes: WinoBias-based edits showing models consistently adopt stereotype-consistent gender presentations." loading="lazy">
              </figure>
              <p class="is-size-7 has-text-grey mt-2 has-text-centered">WinoBias-derived edits with male/female stereotype mapping across occupations.</p>
            </div>
          </div>
          <div class="column is-5">
            <h2 class="title is-3">Gender-Occupation Stereotypes</h2>
            <p>Both models follow occupational stereotypes in <strong>84&ndash;86% of cases</strong>, shifting toward stereotype-consistent gender presentations under gender-coded occupation edits.</p>
            <p class="mt-3">CEO and military prompts push female sources toward male presentations, while nurse and housekeeper prompts push male sources toward female presentations&mdash;indicating occupation-driven gender priors override source identity.</p>
            <p class="mt-3 is-size-6 has-text-grey">We evaluate 50 WinoBias-derived occupation prompts balanced across male- and female-coded roles, with outputs annotated by VLM evaluators and human raters.</p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Gender-Occupation -->

  <!-- Additional Evaluation Screenshots -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <h2 class="title is-3 has-text-centered">Additional Evaluation Interface</h2>
        <div class="columns is-multiline is-variable is-4">
          <div class="column is-half">
            <div class="box has-background-white">
              <figure class="image">
                <img src="static/images/web-task-not-select.png" alt="Evaluation interface before selection, showing both edited outputs awaiting annotation." loading="lazy">
              </figure>
              <p class="is-size-7 has-text-grey mt-2 has-text-centered">Pre-selection view: annotators see the source image, edit prompt, and both edited outputs before making judgments.</p>
            </div>
          </div>
          <div class="column is-half">
            <div class="box has-background-white">
              <figure class="image">
                <img src="static/images/web-task-dashboard-not-done.png" alt="Task dashboard showing incomplete tasks with progress indicators." loading="lazy">
              </figure>
              <p class="is-size-7 has-text-grey mt-2 has-text-centered">Task dashboard (in progress): real-time tracking of annotation completion across all tasks.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Additional Screenshots -->

  <!-- VLM-Human Detailed Alignment -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-vcentered is-variable is-6">
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/vlm_human_alignment.png" alt="VLM-human alignment showing agreement patterns across evaluation dimensions." loading="lazy">
            </figure>
          </div>
          <div class="column is-half">
            <h2 class="title is-3">Detailed VLM-Human Agreement</h2>
            <p>Our dual-VLM ensemble (Gemini 3.0 Flash Preview + GPT-5-mini) produces scores that align closely with human judgments across all five evaluation dimensions.</p>
            <p class="mt-3">The agreement pattern confirms that automated VLM scoring can serve as a reliable proxy for human annotation, enabling scalable evaluation of demographic bias in I2I systems.</p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Detailed Alignment -->

  <!-- VLM-Human Detailed Alignment (Extended) -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-vcentered is-variable is-6">
          <div class="column is-half">
            <h2 class="title is-3">Extended Alignment Analysis</h2>
            <p>Beyond aggregate agreement, we analyze per-dimension and per-demographic alignment to ensure VLM scoring does not systematically misrepresent any subgroup.</p>
            <p class="mt-3">The extended analysis reveals consistent alignment across race, gender, and age categories, confirming that our protocol supports equitable automated evaluation.</p>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/vlm_human_alignment_detailed.png" alt="Extended VLM-human alignment analysis across demographic subgroups." loading="lazy">
            </figure>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Extended Alignment -->

  <!-- Representative Output Mosaic: appendix_k_giant_mosaic.png -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <h2 class="title is-3 has-text-centered">Representative Output Examples</h2>
        <p class="has-text-centered mb-5">A dense mosaic of 480 randomly sampled outputs across all three models, spanning Occupational Stereotype and Vulnerability Attribute prompts across all demographic groups.</p>
        <div class="box has-background-white">
          <figure class="image">
            <img src="static/images/appendix_k_giant_mosaic.png" alt="Dense mosaic of 480 edited outputs from FLUX.2-dev, Step1X-Edit-v1p2, and Qwen-Image-Edit-2511." loading="lazy">
          </figure>
          <p class="is-size-7 has-text-grey mt-2 has-text-centered">Visual inspection reveals pervasive skin lightening and identity drift patterns across models and demographic groups.</p>
        </div>
      </div>
    </div>
  </section>
  <!-- End Mosaic -->

  <!-- Cross-Race Grid: appendix_per_prompt_grid.png -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <h2 class="title is-3 has-text-centered">Cross-Race Comparison</h2>
        <p class="has-text-centered mb-5">The same prompt applied across all seven racial groups enables direct visual comparison of how I2I models treat identical edit requests differently based on source demographics.</p>
        <div class="box has-background-white">
          <figure class="image">
            <img src="static/images/appendix_per_prompt_grid.png" alt="Same prompt across different races: CEO, Doctor, Housekeeper, Politician, Wheelchair User, Aged prompts applied to all seven racial groups." loading="lazy">
          </figure>
          <p class="is-size-7 has-text-grey mt-2 has-text-centered">Each row shows a single prompt applied to all seven racial groups. Notable patterns include consistent skin lightening for darker-skinned subjects and stereotype-congruent feature enhancement.</p>
        </div>
      </div>
    </div>
  </section>
  <!-- End Cross-Race -->

  <!-- Key Findings Summary -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Key Findings</h2>
        <div class="columns is-centered">
          <div class="column is-10">
            <div class="content has-text-left">
              <ul>
                <li><strong>Pervasive Soft Erasure:</strong> Step1X-Edit-v1p2 shows the lowest edit success, reflecting frequent silent non-compliance. Qwen-Image-Edit-2511 achieves the highest edit success (4.65) but FLUX.2-dev exhibits the strongest identity change.</li>
                <li><strong>Systematic skin lightening:</strong> 62&ndash;71% of all edited outputs exhibit lighter skin tones. Indian and Black subjects experience 72&ndash;75% skin lightening vs. 44% for White subjects.</li>
                <li><strong>Asymmetric mitigation:</strong> Feature prompts reduce race change by 1.48 points for Black subjects but only 0.06 for White subjects, revealing a &ldquo;default to White&rdquo; prior in current editors.</li>
                <li><strong>Gender-occupation bias:</strong> Both models follow occupational stereotypes in 84&ndash;86% of cases, overriding source gender with occupation-driven priors.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Findings -->


  <!-- BibTeX -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{seo2026demographic,
  title={Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing},
  author={Huichan Seo and Minki Hong and Sieun Choi and Jihie Kim and Jean Oh},
  journal={arXiv preprint},
  year={2026}
}</code></pre>
    </div>
  </section>
  <!-- End BibTeX -->

  </main>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
  </footer>

</body>
</html>
